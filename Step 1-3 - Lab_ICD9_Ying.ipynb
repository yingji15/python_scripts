{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore whether high dimensional lab values/ICD9 contribute to prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "import scipy as sp\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, roc_auc_score\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know large volumes of data are generated in hospital settings, including clinical and physiological data generated during the course of patient care. Since our goal was to identify early clinical factors or traits useful for predicting the readmission outcome, I want to explore whether including the high dimension lab values and ICD9 values will contribute to better prediction performance.\n",
    "I start with some basic summarizing of lab values/ICD9 data while preserve the high dimension nature of these data. We know high dimension of variables can include lots of noise and be very slow to deal with. Then I try some variable selection to reduce the dimension of data and preserve some useful information within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the patient ID and time window needed \n",
    "We need to predict the readmission event of certain patient, during certain time windows. So we get that informationusing ADT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import adt file, get time window \n",
    "\n",
    "infile9 = \"/Users/ying/Dropbox/Data/FONNESBECK_ADT_20151202.csv\"\n",
    "\n",
    "df_adt = pd.read_csv(infile9, encoding = \"ISO-8859-1\")\n",
    "df_adt = df_adt.loc[df_adt['Event'].isin(['Admit','Discharge'])]\n",
    "df_adt.drop(['SRV_CODE', 'CHIEF_COMPLAINT', 'Event_Date'], axis = 1, inplace = True)\n",
    "df_adt['Admission_date'] = pd.to_datetime(df_adt['Admission_date'])\n",
    "df_adt['DISCHARGE_DATE'] = pd.to_datetime(df_adt['DISCHARGE_DATE'])\n",
    "df_adt.sort_values(['RUID', 'Admission_date', 'DISCHARGE_DATE'], inplace=True)\n",
    "df_adt.head()\n",
    "\n",
    "def clean_table(df):\n",
    "    res = []\n",
    "    for id in df.RUID.unique():\n",
    "        # get the temp dataset for this id\n",
    "        temp = df[df.RUID == id]\n",
    "        # more than 2 like admission discharge admission\n",
    "        if len(temp) > 2:\n",
    "            temp = temp.assign(start_date = None)\n",
    "            temp = temp.assign(end_date = None)\n",
    "            temp = temp.assign(readmission = None)\n",
    "            temp['delta_days'] = (temp.Admission_date.shift(-1) - temp.DISCHARGE_DATE).dt.days\n",
    "            temp['start_date'] = temp.Admission_date.shift(1)\n",
    "            temp['end_date'] = temp.Admission_date.shift(-1)\n",
    "\n",
    "\n",
    "            # only use ADT DISCHARGE row\n",
    "            temp = temp[(temp['Event'] == 'Discharge') & temp['delta_days'] > 0]\n",
    "            temp['readmission'] = (temp['delta_days'] <= 30).astype(int)\n",
    "            temp.drop(['Event', 'Admission_date', 'DISCHARGE_DATE', 'delta_days'], axis = 1, inplace=True)\n",
    "\n",
    "            # now we have temp table have columns: RUID, start_date, end_date, readmission\n",
    "            # append it to res\n",
    "            res.append(temp)\n",
    "    return pd.concat(res)  \n",
    "\n",
    "df = clean_table(df_adt)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lab values\n",
    "\n",
    "The way to deal with lab values: using the mean \"lab value\" of each individual \"lab name\"\n",
    "0) build a empty list to store results\n",
    "1) use dftime to get ruid, start time and end time (time window of that patient)\n",
    "2) use these information to subset the related lab events for this patient and during this time period, group by \"lab name\" and get mean of each group, store this in a list\n",
    "3) concatenate all dataframes in the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import lab file\n",
    "\n",
    "\n",
    "infile5 = \"/Users/ying/Dropbox/Data/FONNESBECK_LAB_20151202.csv\"\n",
    "\n",
    "df_lab = pd.read_csv(infile5, quoting=csv.QUOTE_NONE, encoding=\"ISO-8859-1\")\n",
    "df_lab['Lab_date'] = pd.to_datetime(df_lab['Lab_date'], errors='coerce')\n",
    "df_lab = df_lab.sort_values(['RUID', 'Lab_date'])\n",
    "df_lab['Lab_value'] = df_lab['Lab_value'].str.extract('(\\d+)', expand=False).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start a list to store the part of df_lab selected for each row in df\n",
    "\n",
    "biglist=[]\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    mask = (df_lab['RUID'] == row['RUID']) \\\n",
    "    & (df_lab['Lab_date'] >= row['start_date']) \\\n",
    "    & (df_lab['Lab_date'] <= row['end_date'])\n",
    "    new_row=df_lab.loc[mask,]\n",
    "    \n",
    "    new1=new_row.groupby('Lab_name').mean()\n",
    "    record=np.empty((1,new1.shape[0]))\n",
    "    record[0,]=new1['Lab_value'].values\n",
    "    new12=pd.DataFrame(record, columns=new1.index.tolist())\n",
    "    \n",
    "    biglist.append(new12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#summarize lab values\n",
    "lablist=pd.concat(biglist,axis=0,join=\"outer\")\n",
    "lab=lablist.fillna(0) #25152 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine lab values data with all the other data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine with the clean dataset without individual lab values\n",
    "data = pd.read_csv(\"/Users/ying/Bios8366/BIOS_8366_Proj_Rui_Gillian_Ying/Data/cleandata.csv\")\n",
    "data = data[['readmission', 'Sex', 'Race', 'age', 'Pregnancy_Indicator', \n",
    "             'BMI', 'SYSTOLIC', 'DIASTOLIC', 'EGFR', 'cpt_count', 'cpt_nunique', \n",
    "             'icd_count', 'icd_nunique', 'Lab_value', 'Lab_count', 'drug_count', 'drug_nunique']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup predictors and outcome for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([data,lab],axis=1) #data and lab have same row number, concatenate\n",
    "#df shape: (21252, 2756)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#follow Rui's work in step 3 for modeling\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(['readmission'], axis = 1).values\n",
    "y = df['readmission'].values\n",
    "\n",
    "# replace the missing with median\n",
    "X = Imputer(strategy = 'median').fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic modeling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), \n",
    "        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), \n",
    "        'SGDClassifier': SGDClassifier(), \n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    precision_ = precision_score(ypred, y_test)\n",
    "    recall_ = recall_score(ypred, y_test)\n",
    "    accuracy_ = accuracy_score(ypred,y_test)\n",
    "    f1_ = f1_score(ypred,y_test)\n",
    "    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' \n",
    "          %(name, precision_, recall_, accuracy_, f1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#performance evaluation (output from above)\n",
    "\n",
    "LogisticRegression classifier: precision = 0.6049, recall = 0.6364, accuracy = 0.7177, f1 score = 0.6203\n",
    "\n",
    "SVC classifier: precision = 0.4626, recall = 0.6816, accuracy = 0.7128, f1 score = 0.5511\n",
    "\n",
    "KNeighborsClassifier classifier: precision = 0.5226, recall = 0.5467, accuracy = 0.6529, f1 score = 0.5344\n",
    "\n",
    "GaussianNB classifier: precision = 0.9617, recall = 0.4002, accuracy = 0.4362, f1 score = 0.5652\n",
    "\n",
    "Perceptron classifier: precision = 0.5144, recall = 0.5677, accuracy = 0.6656, f1 score = 0.5397\n",
    "\n",
    "LinearSVC classifier: precision = 0.6070, recall = 0.6208, accuracy = 0.7089, f1 score = 0.6138\n",
    "\n",
    "SGDClassifier classifier: precision = 0.5486, recall = 0.5543, accuracy = 0.6598, f1 score = 0.5514\n",
    "\n",
    "DecisionTreeClassifier classifier: precision = 0.5626, recall = 0.5508, accuracy = 0.6584, f1 score = 0.5566\n",
    "\n",
    "RandomForestClassifier classifier: precision = 0.4819, recall = 0.7397, accuracy = 0.7379, f1 score = 0.5836"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable selection of lab\n",
    "Although this looks better than without lab results, we are concerned about using high dimensions of data.\n",
    "First I wonder what features contribute more. To do it quick and simple, I use the \"feature importance\"in random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#what features are important?\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=4)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "preds = rf.predict(X_test)\n",
    "pd.crosstab(y_test, preds, rownames=['actual'], \n",
    "            colnames=['prediction'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction\t0\t1\n",
    "actual\t\t\n",
    "0\t3420\t526\n",
    "1\t1372\t1058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(df.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, df.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature ranking (selected top 20):\n",
    "1. cpt_nunique (0.028876)\n",
    "2. Lab_count (0.028844)\n",
    "3. cpt_count (0.024849)\n",
    "4. EGFR (0.023369)\n",
    "5. HexacB (0.019920)\n",
    "6. drug_count (0.019067)\n",
    "7. PCP (0.018809)\n",
    "Phencyclidine Screen, Urine Test (3532U)\n",
    "8. RB1 (0.017485)\n",
    "Retinoblastoma (RB1) Testing\n",
    "9. icd_count (0.016888)\n",
    "10. Pregnancy_Indicator (0.016818)\n",
    "11. Lab_value (0.016127)\n",
    "12. SYSTOLIC (0.014930)\n",
    "13. BMI (0.014630)\n",
    "14. Race (0.013513)\n",
    "15. PlsHem (0.013206)\n",
    "HEMOGLOBIN PLASMA\n",
    "16. VtAInt (0.013064)\n",
    "VITAMIN A INTERPRETATION\n",
    "17. BPMPT (0.012998)\n",
    "18. CYT-ST (0.011991)\n",
    "Cytogenetics Studies\n",
    "19. AMTSBH (0.011770)\n",
    "AMOUNT SAMPLE  HELD\n",
    "20. Alanin (0.011674)\n",
    "ALANINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this was using all lab data. So our dataset was 21252 rows and 2756 columns. \n",
    "We want to get some dimension reduction for the following reasons:\n",
    "\n",
    "1) It helps in data compressing and reducing the storage space required, now the data is >200 MB.\n",
    "\n",
    "2) It fastens the time required for performing same computations. Less dimensions leads to less computing, also less dimensions can allow usage of algorithms unfit for a large number of dimensions\n",
    "\n",
    "3) It removes redundant features\n",
    "\n",
    "To start with, I tried a simple PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use 100 PC from lab + clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, whiten=True).fit(lab)\n",
    "lab_pca = pca.transform(lab)\n",
    "\n",
    "lab3=pd.DataFrame(lab_pca)\n",
    "\n",
    "df100=pd.concat([data,lab3],axis=1) #df100 shape: (21252, 117)\n",
    "\n",
    "X = df100.drop(['readmission'], axis = 1).values\n",
    "y = df100['readmission'].values\n",
    "\n",
    "# replace the missing with median\n",
    "X = Imputer(strategy = 'median').fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modeling and evaluation\n",
    "\n",
    "clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), \n",
    "        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), \n",
    "        'SGDClassifier': SGDClassifier(), \n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    precision_ = precision_score(ypred, y_test)\n",
    "    recall_ = recall_score(ypred, y_test)\n",
    "    accuracy_ = accuracy_score(ypred,y_test)\n",
    "    f1_ = f1_score(ypred,y_test)\n",
    "    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' \n",
    "          %(name, precision_, recall_, accuracy_, f1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#performance evaluated (output from above)\n",
    "\n",
    "LogisticRegression classifier: precision = 0.3498, recall = 0.5886, accuracy = 0.6590, f1 score = 0.4388\n",
    "\n",
    "SVC classifier: precision = 0.3123, recall = 0.6161, accuracy = 0.6637, f1 score = 0.4145\n",
    "\n",
    "KNeighborsClassifier classifier: precision = 0.4835, recall = 0.5154, accuracy = 0.6299, f1 score = 0.4989\n",
    "\n",
    "GaussianNB classifier: precision = 0.9778, recall = 0.3843, accuracy = 0.3944, f1 score = 0.5517\n",
    "\n",
    "Perceptron classifier: precision = 0.6440, recall = 0.5128, accuracy = 0.6311, f1 score = 0.5710\n",
    "\n",
    "LinearSVC classifier: precision = 0.2988, recall = 0.5893, accuracy = 0.6534, f1 score = 0.3965\n",
    "\n",
    "SGDClassifier classifier: precision = 0.3222, recall = 0.4845, accuracy = 0.6110, f1 score = 0.3870\n",
    "\n",
    "DecisionTreeClassifier classifier: precision = 0.5140, recall = 0.5081, accuracy = 0.6252, f1 score = 0.5110\n",
    "\n",
    "RandomForestClassifier classifier: precision = 0.4379, recall = 0.6642, accuracy = 0.7014, f1 score = 0.5278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use 20 PC from lab + clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20, whiten=True).fit(lab)\n",
    "lab_pca = pca.transform(lab)\n",
    "\n",
    "lab2=pd.DataFrame(lab_pca)\n",
    "\n",
    "df20=pd.concat([data,lab3],axis=1) #df20 shape: (21252, 36)\n",
    "\n",
    "X = df20.drop(['readmission'], axis = 1).values\n",
    "y = df20['readmission'].values\n",
    "\n",
    "# replace the missing with median\n",
    "X = Imputer(strategy = 'median').fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modeling and evaluation\n",
    "\n",
    "clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), \n",
    "        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), \n",
    "        'SGDClassifier': SGDClassifier(), \n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    precision_ = precision_score(ypred, y_test)\n",
    "    recall_ = recall_score(ypred, y_test)\n",
    "    accuracy_ = accuracy_score(ypred,y_test)\n",
    "    f1_ = f1_score(ypred,y_test)\n",
    "    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' \n",
    "          %(name, precision_, recall_, accuracy_, f1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression classifier: precision = 0.3498, recall = 0.5886, accuracy = 0.6590, f1 score = 0.4388\n",
    "\n",
    "SVC classifier: precision = 0.3123, recall = 0.6161, accuracy = 0.6637, f1 score = 0.4145\n",
    "\n",
    "KNeighborsClassifier classifier: precision = 0.4835, recall = 0.5154, accuracy = 0.6299, f1 score = 0.4989\n",
    "\n",
    "GaussianNB classifier: precision = 0.9778, recall = 0.3843, accuracy = 0.3944, f1 score = 0.5517\n",
    "\n",
    "Perceptron classifier: precision = 0.6440, recall = 0.5128, accuracy = 0.6311, f1 score = 0.5710\n",
    "\n",
    "LinearSVC classifier: precision = 0.2975, recall = 0.5859, accuracy = 0.6521, f1 score = 0.3947\n",
    "\n",
    "SGDClassifier classifier: precision = 0.3148, recall = 0.4567, accuracy = 0.5961, f1 score = 0.3727\n",
    "\n",
    "DecisionTreeClassifier classifier: precision = 0.5152, recall = 0.5161, accuracy = 0.6311, f1 score = 0.5157\n",
    "\n",
    "RandomForestClassifier classifier: precision = 0.4383, recall = 0.6554, accuracy = 0.6981, f1 score = 0.5253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#missing values in start date\n",
    "data0 = pd.read_csv(\"/Users/ying/Bios8366/BIOS_8366_Proj_Rui_Gillian_Ying/Data/cleandata.csv\")\n",
    "\n",
    "dftime = data0[['RUID', 'start_date','end_date']]\n",
    "dftime.shape #shape is (21252,3), includes 3 columns for each row: ruid, start time and end time, \n",
    "#we use these to get the time widow for further filtering\n",
    "dftime['start_date'].isnull().sum() #there are 82 missing values in start date column\n",
    "dftime['start_date']=dftime['start_date'].fillna(dftime['end_date'])\n",
    "#if the start date is missing, i use the end date to fill \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import ICD9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in icd9\n",
    "# ICD9\n",
    "\n",
    "infile7 = \"../../Data/FONNESBECK_ICD9_20151202.csv\"\n",
    "\n",
    "df_icd = pd.read_csv(infile7)\n",
    "df_icd['Event_date'] = pd.to_datetime(df_icd['Event_date'])\n",
    "df_icd = df_icd.sort_values(['RUID', 'Event_date'])\n",
    "df_icd['ICD9_Code'].replace({'000.00': None}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I try to deal with ICD9: using counts of individual ICD9 codes\n",
    "0) build a empty list to store results\n",
    "1) use dftime to get ruid, start time and end time (time window of that patient)\n",
    "2) use these information to subset the related icd9 events for this patient and during this time period, store this in a list\n",
    "3) concatenate all dataframes in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deal with icd9, similar idea: for each row in dftime (get time window), build a list to store the dataframe generated for each row\n",
    "icdlist=[]\n",
    "#n=0\n",
    "\n",
    "for index, row in dftime.iterrows():    \n",
    "    mask = (df_icd['RUID'] == row['RUID']) \\   #same id \n",
    "    & (df_icd['Event_date'] >= row['start_date']) \\ #filter by time\n",
    "    & (df_icd['Event_date'] <= row['end_date'])\n",
    "    \n",
    "    new_row=df_icd.loc[mask,] #subset the part of df_icd meeting the criteria above\n",
    "    \n",
    "    temp1=new_row.groupby('ICD9_Code').agg('count') #use ICD9 code to group the dataframe, then count occurence of each ICD9 code\n",
    "    temp2=temp1[['RUID']] #select 1 column (both have same count)\n",
    "    record=np.empty((1,temp2.shape[0])) #construct empty array to store \"transformed\" data\n",
    "    record[0,]=temp2['RUID'].values #keep values in record\n",
    "    new12=pd.DataFrame(record, columns=temp2.index.tolist()) #construct a dataframe, with values in record and index (ICD9 code) keeped\n",
    "    \n",
    "    \n",
    "    icdlist.append(new12) #use the list to store dataframe generated in each step\n",
    "    \n",
    "    #n+=1\n",
    "    \n",
    "    #if n>10 : (for testing the code, just ignore)\n",
    "     #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "icdsumm=pd.concat(icdlist,axis=0,join=\"outer\") #outer join the dataframes saved in the list, all values and index keeped\n",
    "icdsum2=icdsumm.fillna(0) #there are lots of NA generated by outer join, fill with 0 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable reduction of lCD9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this looks better than without lab results, we are concerned about using high dimensions of data.\n",
    "First I wonder what features contribute more. To do it quick and simple, I use the \"feature importance\"in random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### variable reduction using PCA, 20 PCs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20, whiten=True).fit(icdsum2)\n",
    "icd_pca = pca.transform(icdsum2)\n",
    "icd20=pd.DataFrame(icd_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### variable reduction using PCA, 100 PCs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, whiten=True).fit(icdsum2)\n",
    "icd_pca = pca.transform(icdsum2)\n",
    "icd100=pd.DataFrame(icd_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine top 20 PC from ICD9, top 20 PC from lab with all other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dflabicd20=pd.concat([df20,icd20],axis=1)\n",
    "\n",
    "X = dflabicd20.drop(['readmission'], axis = 1).values\n",
    "y = dflabicd20['readmission'].values\n",
    "\n",
    "# replace the missing with median\n",
    "X = Imputer(strategy = 'median').fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), \n",
    "        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), \n",
    "        'SGDClassifier': SGDClassifier(), \n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    precision_ = precision_score(ypred, y_test)\n",
    "    recall_ = recall_score(ypred, y_test)\n",
    "    accuracy_ = accuracy_score(ypred,y_test)\n",
    "    f1_ = f1_score(ypred,y_test)\n",
    "    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' \n",
    "          %(name, precision_, recall_, accuracy_, f1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance output:\n",
    "\n",
    "LogisticRegression classifier: precision = 0.4379, recall = 0.6154, accuracy = 0.6815, f1 score = 0.5117\n",
    "\n",
    "SVC classifier: precision = 0.4029, recall = 0.6766, accuracy = 0.6990, f1 score = 0.5050\n",
    "\n",
    "KNeighborsClassifier classifier: precision = 0.5317, recall = 0.5694, accuracy = 0.6683, f1 score = 0.5499\n",
    "\n",
    "GaussianNB classifier: precision = 0.9621, recall = 0.3952, accuracy = 0.4244, f1 score = 0.5603\n",
    "\n",
    "Perceptron classifier: precision = 0.5041, recall = 0.5121, accuracy = 0.6280, f1 score = 0.5081\n",
    "\n",
    "LinearSVC classifier: precision = 0.3885, recall = 0.6215, accuracy = 0.6768, f1 score = 0.4781\n",
    "\n",
    "SGDClassifier classifier: precision = 0.5498, recall = 0.5072, accuracy = 0.6248, f1 score = 0.5276\n",
    "\n",
    "DecisionTreeClassifier classifier: precision = 0.5444, recall = 0.5442, accuracy = 0.6526, f1 score = 0.5443\n",
    "\n",
    "RandomForestClassifier classifier: precision = 0.4481, recall = 0.7179, accuracy = 0.7226, f1 score = 0.5518"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine top 100 PC from ICD9, top 100 PC from lab with all other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dflabicd100=pd.concat([df100,icd100],axis=1)\n",
    "\n",
    "X = dflabicd100.drop(['readmission'], axis = 1).values\n",
    "y = dflabicd100['readmission'].values\n",
    "\n",
    "# replace the missing with median\n",
    "X = Imputer(strategy = 'median').fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'LogisticRegression':LogisticRegression(),'SVC': SVC(), \n",
    "        'KNeighborsClassifier': KNeighborsClassifier(n_neighbors = 3),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'Perceptron': Perceptron(), 'LinearSVC': LinearSVC(), \n",
    "        'SGDClassifier': SGDClassifier(), \n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "        'RandomForestClassifier': RandomForestClassifier(n_estimators=100)}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    precision_ = precision_score(ypred, y_test)\n",
    "    recall_ = recall_score(ypred, y_test)\n",
    "    accuracy_ = accuracy_score(ypred,y_test)\n",
    "    f1_ = f1_score(ypred,y_test)\n",
    "    print('%s classifier: precision = %.4f, recall = %.4f, accuracy = %.4f, f1 score = %.4f' \n",
    "          %(name, precision_, recall_, accuracy_, f1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance output\n",
    "\n",
    "LogisticRegression classifier: precision = 0.5556, recall = 0.6490, accuracy = 0.7161, f1 score = 0.5987\n",
    "SVC classifier: precision = 0.5107, recall = 0.6826, accuracy = 0.7230, f1 score = 0.5843\n",
    "KNeighborsClassifier classifier: precision = 0.5498, recall = 0.5656, accuracy = 0.6675, f1 score = 0.5576\n",
    "GaussianNB classifier: precision = 0.9309, recall = 0.4004, accuracy = 0.4424, f1 score = 0.5600\n",
    "Perceptron classifier: precision = 0.6103, recall = 0.5381, accuracy = 0.6518, f1 score = 0.5719\n",
    "LinearSVC classifier: precision = 0.5407, recall = 0.6486, accuracy = 0.7133, f1 score = 0.5898\n",
    "SGDClassifier classifier: precision = 0.5453, recall = 0.5439, accuracy = 0.6524, f1 score = 0.5446\n",
    "DecisionTreeClassifier classifier: precision = 0.5342, recall = 0.5200, accuracy = 0.6346, f1 score = 0.5270\n",
    "RandomForestClassifier classifier: precision = 0.4107, recall = 0.7301, accuracy = 0.7175, f1 score = 0.5257"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
